{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import time \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "#if module_path not in sys.path:\n",
    "sys.path.append(module_path+\"/test_perf\")\n",
    "\n",
    "    \n",
    "from src.init_runtime import init_runtime\n",
    "from main import ROOT_DIR\n",
    "from test_perf._model_wrappers import EfrsLocal, ModelWrapperBase, EfrsRestApi\n",
    "from test_perf.dto import Dataset\n",
    "from test_perf.dto import Image, Name\n",
    "from test_perf.recognition_accuracy_test import get_lfw_dataset, get_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from src import pyutils\n",
    "from collections import namedtuple\n",
    "Calculator = namedtuple('Calculator', 'graph sess')\n",
    "import tensorflow as tf\n",
    "from src.storage.constants import EMBEDDING_CALCULATOR_MODEL_FILENAME\n",
    "from src.storage.storage import get_storage\n",
    "CALCULATOR_VERSION = EMBEDDING_CALCULATOR_MODEL_FILENAME\n",
    "BATCH_SIZE = 25\n",
    "from src.face_recognition.dto.embedding import Embedding\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "@pyutils.run_once\n",
    "def _calculator() -> Calculator:\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(get_storage().get_file(CALCULATOR_VERSION))\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "        return Calculator(graph=graph, sess=tf.Session(graph=graph))\n",
    "\n",
    "\n",
    "def _calculate_embeddings(cropped_images):\n",
    "    \"\"\"\n",
    "    Quick fix for a bug where it can't handle many cropped_images\n",
    "    is to give one image at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(get_storage().get_file(CALCULATOR_VERSION))\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "        calculator = Calculator(graph=graph, sess=tf.Session(graph=graph))\n",
    "    \n",
    "        # Get tensors and constants\n",
    "    images_placeholder = calculator.graph.get_tensor_by_name(\"input:0\")\n",
    "    embeddings = calculator.graph.get_tensor_by_name(\"embeddings:0\")\n",
    "    phase_train_placeholder = calculator.graph.get_tensor_by_name(\"phase_train:0\")\n",
    "    embedding_size = embeddings.get_shape()[1]\n",
    "\n",
    "    # Run forward pass to calculate embeddings\n",
    "    image_count = len(cropped_images)\n",
    "    batches_per_epoch = int(math.ceil(1.0 * image_count / BATCH_SIZE))\n",
    "    emb_array = np.zeros((image_count, embedding_size))\n",
    "    for i in range(batches_per_epoch):\n",
    "        start_index = i * BATCH_SIZE\n",
    "        end_index = min((i + 1) * BATCH_SIZE, image_count)\n",
    "        feed_dict = {images_placeholder: cropped_images, phase_train_placeholder: False}\n",
    "        emb_array[start_index:end_index, :] = calculator.sess.run(embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    # Return DTO\n",
    "    return [Embedding(array=emb, calculator_version=CALCULATOR_VERSION) for emb in emb_array]\n",
    "\n",
    "def calculate_embedding(image: np.ndarray) -> Embedding:\n",
    "    return _calculate_embeddings([image])[0]\n",
    "\n",
    "def calculate_embeddings(cropped_images: List[np.ndarray]) -> List[Embedding]:\n",
    "    return [calculate_embedding(image) for image in cropped_images]\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from http import HTTPStatus\n",
    "\n",
    "import requests\n",
    "\n",
    "#from src.face_recognition.calc_embedding.calculator import calculate_embeddings\n",
    "from src.face_recognition.classify_embedding.predict import predict_from_image_with_classifier\n",
    "from src.face_recognition.classify_embedding.train import get_trained_classifier\n",
    "from src.face_recognition.crop_faces.crop_faces import crop_one_face\n",
    "from src.face_recognition.crop_faces.exceptions import NoFaceFoundError\n",
    "from src.pyutils.serialization import numpy_to_jpg_file\n",
    "from test_perf.dto import Image, Name\n",
    "\n",
    "\n",
    "class ModelWrapperBase(ABC):\n",
    "    @abstractmethod\n",
    "    def add_face_example(self, img: Image, name: Name):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def recognize(self, img: Image) -> Name:\n",
    "        pass\n",
    "\n",
    "\n",
    "class EfrsLocal(ModelWrapperBase):\n",
    "    def __init__(self):\n",
    "        self._cropped_images = []\n",
    "        self._names = []\n",
    "        self._classifier = None\n",
    "\n",
    "    def add_face_example(self, img: Image, name: Name):\n",
    "        try:\n",
    "            cropped_img = crop_one_face(img).img\n",
    "        except NoFaceFoundError as e:\n",
    "            logging.warning(f\"Failed to add face example. Skipping. {str(e)}\")\n",
    "            return 1\n",
    "        self._cropped_images.append(cropped_img)\n",
    "        self._names.append(name)\n",
    "        return 0\n",
    "\n",
    "    def train(self):\n",
    "        embeddings = calculate_embeddings(self._cropped_images)\n",
    "        self._classifier = get_trained_classifier(embeddings, self._names)\n",
    "\n",
    "    def recognize(self, img: Image) -> Name:\n",
    "        try:\n",
    "            predictions = predict_from_image_with_classifier(img=img, classifier=self._classifier, limit=1)\n",
    "        except NoFaceFoundError as e:\n",
    "            logging.warning(f\"Face is not found in the image to be recognized. Skipping. {str(e)}\")\n",
    "            return ''\n",
    "        return predictions[0].face_name    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TEST_CODE = ROOT_DIR / 'test_perf'\n",
    "class mock_model(ModelWrapperBase):\n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "\n",
    "\n",
    "    def add_face_example(self, img: Image, name: Name):\n",
    "        time.sleep(random.random())\n",
    "        return random.choice([0,0, 1])\n",
    "\n",
    "    def train(self):\n",
    "        time.sleep(random.random())\n",
    "\n",
    "\n",
    "    def recognize(self, name):\n",
    "        time.sleep(random.random())\n",
    "        rand = random.choice([0, 1, 1, 1])\n",
    "        logging.debug(rand)\n",
    "        if rand == 1:\n",
    "            return name\n",
    "        elif rand == 0:\n",
    "            return \"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# function that returns 1. number of detected faces for both models, 2. how much total time it took to add them,\n",
    "# 3. time to train both models \n",
    "# 4. lists for the undetected faces from the ones that were attempted to be added\n",
    "def calculate_detection(model_1: ModelWrapperBase, dataset: Dataset, model_2: ModelWrapperBase, dataset_size): \n",
    "    undetected_1 = 0\n",
    "    list_to_remove_1 = []\n",
    "    undetected_2 = 0\n",
    "    list_to_remove_2 = []\n",
    "\n",
    "    add_time_model_1 = 0\n",
    "    add_time_model_2 = 0\n",
    "    \n",
    "    unique_detection = 0\n",
    "    prev_name = None\n",
    "    \n",
    "    count_1 = 0 \n",
    "    count_2 = 0\n",
    "    \n",
    "    while unique_detection <= dataset_size:\n",
    "        for name, img in dataset.train:\n",
    "            # for both: insure that we only train with one picture and will be testing only with one picture\n",
    "            if name == prev_name:\n",
    "                prev_name = name\n",
    "                continue\n",
    "            else: \n",
    "                unique_detection += 1\n",
    "                prev_name = name\n",
    "\n",
    "\n",
    "            if count_1 < dataset_size:\n",
    "                # for the first model \n",
    "                # step 1: find the time of the start\n",
    "                start_1 = time.time()\n",
    "                # step 2: try to add face (or it was not detected --> was_not_detected ==1 else 0)\n",
    "                was_not_detected = model_1.add_face_example(img, name)\n",
    "                # step 3: find the end time\n",
    "                end_1 = time.time()\n",
    "                # step 4: find the time it took to add the picture \n",
    "                add_time_model_1 += (end_1 - start_1)\n",
    "                # step 5: if was not detected add the count and add to the list to remove from dataset.test\n",
    "                undetected_1 += was_not_detected\n",
    "                if was_not_detected:\n",
    "                    list_to_remove_1.append(name)\n",
    "                count_1 += 1\n",
    "                    \n",
    "            if count_2 < dataset_size:\n",
    "                # for the second model\n",
    "                start_2 = time.time()\n",
    "                was_not_detected = model_2.add_face_example(img, name)\n",
    "                print (was_not_detected)\n",
    "                end_2 = time.time()\n",
    "                add_time_model_2 += (end_2 - start_2)\n",
    "                if was_not_detected:\n",
    "                    list_to_remove_2.append(name)\n",
    "            \n",
    "                undetected_2 += was_not_detected\n",
    "                \n",
    "                count_2 +=1 \n",
    "        \n",
    "    start_train_1 = time.time()\n",
    "    model_1.train()\n",
    "    end_train_1 = time.time()\n",
    "    \n",
    "    total_trian_time_1 = end_train_1 - start_train_1\n",
    "    \n",
    "    start_train_2 = time.time()\n",
    "    model_2.train()\n",
    "    end_train_2 = time.time()\n",
    "    \n",
    "    total_trian_time_2 = end_train_2 - start_train_2\n",
    "    \n",
    "    return undetected_1, undetected_2, list_to_remove_1, list_to_remove_2, \\\n",
    "add_time_model_1, add_time_model_2, total_trian_time_1, total_trian_time_2\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_models(model_1: ModelWrapperBase, dataset: Dataset, model_2: ModelWrapperBase, dataset_size, list_to_remove_1, list_to_remove_2):  \n",
    "    \n",
    "    recognized_1 = 0\n",
    "    recognized_2 = 0\n",
    "\n",
    "    recognition_time_1 = 0\n",
    "    recognition_time_2 = 0\n",
    "    \n",
    "    count_1 = 0 \n",
    "    count_2 = 0 \n",
    "    \n",
    "    prev_name_1 = None\n",
    "    prev_name_2 = None\n",
    "    \n",
    "    # for every element in the test dataset \n",
    "    for name, img in dataset.test:\n",
    "        # if we reached the needed number of faces for both models, we can break out \n",
    "        if count_1 > dataset_size and count_2 > dataset_size:\n",
    "            break\n",
    "        # for the 2 models\n",
    "        for model in range (2):\n",
    "            # for first one \n",
    "            if model == 0:\n",
    "                # check if we are already there in the count\n",
    "                if count_1 <= dataset_size: \n",
    "                    # if the face was not detected by the model before, then it should not be recognized now\n",
    "                    if name in list_to_remove_1:\n",
    "                        prev_name_1 = name\n",
    "                        continue\n",
    "                    # if we already attempted to recognize this face in another picture, move on (inforces only one test)\n",
    "                    elif name == prev_name_1:\n",
    "                        prev_name_1 = name\n",
    "                        continue\n",
    "                    # otherwise try recognizing \n",
    "                    else:\n",
    "                        start_recogn_1 = time.time()\n",
    "                        logging.debug(start_recogn_1)\n",
    "                        # if we actually recognized it right\n",
    "                        if name == model_1.recognize(img): # remove name param when not testing the mock\n",
    "                            # add to the recognized count \n",
    "                            recognized_1 += 1\n",
    "                        end_recogn_1 =  time.time()\n",
    "                        # add to the recognition time \n",
    "                        recognition_time_1 += (end_recogn_1 - start_recogn_1) \n",
    "                        prev_name_1 = name\n",
    "                        count_1+= 1\n",
    "            # do the same for the other model \n",
    "            elif model == 1:\n",
    "                if count_2 <= dataset_size: \n",
    "                    if name in list_to_remove_2:\n",
    "                        prev_name_2 = name\n",
    "                        continue\n",
    "                    elif name == prev_name_2:\n",
    "                        prev_name_2 = name\n",
    "                        continue\n",
    "                    else:\n",
    "                        start_recogn_2 = time.time()\n",
    "                        logging.debug(start_recogn_2)\n",
    "                        if name == model_2.recognize(name):\n",
    "                            recognized_2 += 1\n",
    "                        end_recogn_2 = time.time()\n",
    "                        logging.debug(end_recogn_2)\n",
    "                        recognition_time_2 += (end_recogn_2 - start_recogn_2) \n",
    "                        prev_name_2 = name\n",
    "                        count_2 += 1\n",
    "    return recognition_time_1, recognition_time_2, recognized_1, recognized_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "plt.rcParams['figure.figsize'] = [9, 9]\n",
    "\n",
    "def build_graph(dict):\n",
    "    \n",
    "    width_ = 100\n",
    "    fig, ax = plt.subplots()\n",
    "    x = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    for key in dict:\n",
    "        x.append(key)\n",
    "\n",
    "        y1.append(dict[key][0])\n",
    "        y2.append(dict[key][1])\n",
    "    x = np.asarray(x)\n",
    "    y1 = np.asarray(y1)\n",
    "    y2 = np.asarray(y2)\n",
    "    print(x)\n",
    "    print(y1)\n",
    "    print(y2)\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    #width = np.diff(x)\n",
    "    #width = [  90 , 900, 9000, 90000]\n",
    "    #width = [9, 90, 900, 9000]\n",
    "    width = []\n",
    "    for i in range (len(x)):\n",
    "        number = x[i]-10**(i)\n",
    "        width.append(number)\n",
    "                     \n",
    "    rects1 = plt.bar(x, y1, width, color='#028482', error_kw=error_config, label='Model #1', ec=\"k\")\n",
    "    rects2 = plt.bar(x + width, y2, width, color='#7ABA7A', error_kw=error_config, label='Model #2', ec=\"k\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylabel('% recognized')\n",
    "    ax.set_title('Scores by size of dataset and model')\n",
    "    width = np.asarray(width)\n",
    "    ax.set_xticks(x + width / 2)\n",
    "    ax.set_xticklabels(x)\n",
    "\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    # plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#mock_result = {10: (74, 98), 100: (78, 90), 1000: (81, 95), 10000: (80, 90)}\n",
    "#build_graph(mock_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/dotenv/main.py:52: UserWarning: File doesn't exist \n",
      "  warnings.warn(\"File doesn't exist {}\".format(self.dotenv_path))  # type: ignore\n",
      "DEBUG:root:Using MongoDB at localhost:27017\n",
      "usage: ipykernel_launcher.py [-h] [--host HOST] [--test]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/elizabeth/Library/Jupyter/runtime/kernel-59ea3a4c-dc9f-4672-93b6-bc85d2c0ed95.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ],
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "def main_test(test, host):\n",
    "    \n",
    "    init_runtime()\n",
    "    dataset = get_test_dataset() if test else get_lfw_dataset()\n",
    "    model_1 = EfrsRestApi(host) if host else EfrsLocal()\n",
    "    #model_1 = EfrsLocal()\n",
    "    #model_2 = EfrsLocal()\n",
    "    #model_1 = mock_model(dataset)\n",
    "    model_2 = mock_model(dataset)\n",
    "\n",
    "    \n",
    "    size_1 = 10\n",
    "    size_2 = 100\n",
    "    size_3 = 1000\n",
    "    size_4 = 10000\n",
    "    sizes = [size_1] #size_2, size_3, size_4\n",
    "    \n",
    "    recognized_dict = {}\n",
    "    for size in sizes:\n",
    "        undetected_1, undetected_2, list_to_remove_1, list_to_remove_2, \\\n",
    "add_time_model_1, add_time_model_2, total_trian_time_1, total_trian_time_2 = calculate_detection(model_1, dataset, model_2, size)\n",
    "        print (undetected_1, undetected_2)\n",
    "        detected_1 = size - undetected_1\n",
    "        detected_percent_1 = (detected_1 / size) * 100 \n",
    "        detected_2 = size - undetected_2\n",
    "        detected_percent_2 = (detected_2 / size) * 100 \n",
    "        recognition_time_1, recognition_time_2, recognized_1, recognized_2 = test_models(model_1, dataset, model_2, size, list_to_remove_1, list_to_remove_2)\n",
    "        recognized_percent_1 = (recognized_1 / size) * 100 \n",
    "        recognized_percent_2 = (recognized_2 / size) * 100\n",
    "        recognized_dict[size] = (recognized_1, recognized_2)\n",
    "     \n",
    "        \n",
    "        \n",
    "        print(f'==================\\n'\n",
    "              f'Model #1 Performance: \\n'\n",
    "              f'Detected faces: {detected_percent_1}% ({detected_1}/{size})\\n'\n",
    "              f'Total time for adding faces: {add_time_model_1}\\n'\n",
    "              f'Total time for training model: {total_trian_time_1}\\n'\n",
    "              f'------------------\\n'\n",
    "              f'Recognized faces: {recognized_percent_1}% ({recognized_1}/{size})\\n'\n",
    "              f'Total time for recognizing faces: {recognition_time_2}\\n')\n",
    "        \n",
    "        print(f'==================\\n'\n",
    "              f'Model #2 Performance: \\n'\n",
    "              f'Detected faces: {detected_percent_2}% ({detected_2}/{size})\\n'\n",
    "              f'Total time for adding faces: {add_time_model_2}\\n'\n",
    "              f'Total time for training model: {total_trian_time_2}\\n'\n",
    "              f'------------------\\n'\n",
    "              f'Recognized faces: {recognized_percent_2}% ({recognized_2}/{size})\\n'\n",
    "              f'Total time for recognizing faces: {recognition_time_2}\\n')\n",
    "        \n",
    "        \n",
    "        build_graph(recognized_dict)\n",
    "    \n",
    "  \n",
    "host = None # put the host here to use the EfrsRestApi\n",
    "test = False \n",
    "main_test(test, host)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}